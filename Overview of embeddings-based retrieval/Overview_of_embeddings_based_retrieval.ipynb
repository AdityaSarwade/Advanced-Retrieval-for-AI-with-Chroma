{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ec889d86-0d16-477f-8b7f-be03d73ad957",
      "metadata": {
        "id": "ec889d86-0d16-477f-8b7f-be03d73ad957"
      },
      "source": [
        "# Lab 1 - Overview of embeddings-based retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58ee2f53-d88b-4f00-94a2-75a66d4149e9",
      "metadata": {
        "id": "58ee2f53-d88b-4f00-94a2-75a66d4149e9"
      },
      "source": [
        "Welcome! Here's a few notes about the Chroma course notebooks.\n",
        " - A number of warnings pop up when running the notebooks. These are normal and can be ignored.\n",
        " - Some operations such as calling an LLM or an operation using generated data return unpredictable results and so your notebook outputs may differ from the video.\n",
        " - These custom notebooks substitute GPT 3.5 Turbo for an open source LLM by Stability AI called StableBeluga-7B (Be sure to run the notebook on a CUDA enabled device)\n",
        " - If using Colab connect to the T4 GPU instance\n",
        "  \n",
        "Enjoy the course!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --quiet"
      ],
      "metadata": {
        "id": "vuPHtqjVBLku"
      },
      "id": "vuPHtqjVBLku",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a5536f0-651c-40e7-aa15-27ee0cda80b7",
      "metadata": {
        "height": 30,
        "id": "5a5536f0-651c-40e7-aa15-27ee0cda80b7"
      },
      "outputs": [],
      "source": [
        "from helper_utils import word_wrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3748b16d-d4a7-49c3-a48a-57dcfc42acd6",
      "metadata": {
        "height": 166,
        "id": "3748b16d-d4a7-49c3-a48a-57dcfc42acd6"
      },
      "outputs": [],
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "reader = PdfReader(\"microsoft_annual_report_2022.pdf\")\n",
        "pdf_texts = [p.extract_text().strip() for p in reader.pages]\n",
        "\n",
        "# Filter the empty strings\n",
        "pdf_texts = [text for text in pdf_texts if text]\n",
        "\n",
        "print(word_wrap(pdf_texts[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a338ec83-6301-41a5-9ab1-e5d583306a3f",
      "metadata": {
        "height": 47,
        "id": "a338ec83-6301-41a5-9ab1-e5d583306a3f"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "888a86f8-2fe2-4682-bdaf-c15129ed1a32",
      "metadata": {
        "height": 166,
        "id": "888a86f8-2fe2-4682-bdaf-c15129ed1a32"
      },
      "outputs": [],
      "source": [
        "character_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=0\n",
        ")\n",
        "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
        "\n",
        "print(word_wrap(character_split_texts[10]))\n",
        "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5665c695-22ea-4264-b1ac-5ba720b6d78b",
      "metadata": {
        "height": 149,
        "id": "5665c695-22ea-4264-b1ac-5ba720b6d78b"
      },
      "outputs": [],
      "source": [
        "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
        "\n",
        "token_split_texts = []\n",
        "for text in character_split_texts:\n",
        "    token_split_texts += token_splitter.split_text(text)\n",
        "\n",
        "print(word_wrap(token_split_texts[10]))\n",
        "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a13d14-4484-46f0-8e67-277337f9d138",
      "metadata": {
        "height": 98,
        "id": "c2a13d14-4484-46f0-8e67-277337f9d138"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
        "\n",
        "embedding_function = SentenceTransformerEmbeddingFunction()\n",
        "print(embedding_function([token_split_texts[10]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba6c8c5-9ce4-44d0-9223-6fdd77871f87",
      "metadata": {
        "height": 132,
        "id": "8ba6c8c5-9ce4-44d0-9223-6fdd77871f87"
      },
      "outputs": [],
      "source": [
        "chroma_client = chromadb.Client()\n",
        "chroma_collection = chroma_client.create_collection(\"microsoft_annual_report_2022\", embedding_function=embedding_function)\n",
        "\n",
        "ids = [str(i) for i in range(len(token_split_texts))]\n",
        "\n",
        "chroma_collection.add(ids=ids, documents=token_split_texts)\n",
        "chroma_collection.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfdb54db-a442-423c-b006-c33a257cd7d7",
      "metadata": {
        "height": 149,
        "id": "bfdb54db-a442-423c-b006-c33a257cd7d7"
      },
      "outputs": [],
      "source": [
        "query = \"What was the total revenue?\"\n",
        "\n",
        "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
        "retrieved_documents = results['documents'][0]\n",
        "\n",
        "for document in retrieved_documents:\n",
        "    print(word_wrap(document))\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "\n",
        "# Bits and bytes config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/StableBeluga-7B\", use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"stabilityai/StableBeluga-7B\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\", quantization_config=bnb_config)"
      ],
      "metadata": {
        "id": "5WbPjMAGTAbz"
      },
      "id": "5WbPjMAGTAbz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag(query, retrieved_documents, model=model, tokenizer=tokenizer):\n",
        "    # get the start time\n",
        "    st = time.time()\n",
        "    information = \"\\n\\n\".join(retrieved_documents)\n",
        "\n",
        "    system_prompt = \"### System: You are a helpful expert financial research assistant. Your users are asking questions about information contained in an annual report. You will be shown the user's question, and the relevant information from the annual report. Answer the user's question using only this information.\\n\\n\"\n",
        "    prompt = system_prompt + f\"### User:\\nQuestion: {query}.\\nInformation: {information}\\n\\n### Assistant:\\n\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=2048)\n",
        "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # get the end time\n",
        "    et = time.time()\n",
        "\n",
        "    # get the execution time\n",
        "    elapsed_time = et - st\n",
        "\n",
        "    return output, elapsed_time"
      ],
      "metadata": {
        "id": "zy8aihT1TKHh"
      },
      "id": "zy8aihT1TKHh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28bac3a2-0d29-48dc-9b48-2d9313239a25",
      "metadata": {
        "height": 64,
        "id": "28bac3a2-0d29-48dc-9b48-2d9313239a25"
      },
      "outputs": [],
      "source": [
        "output, elapsed_time = rag(query=query, retrieved_documents=retrieved_documents)\n",
        "\n",
        "print('Execution time:', elapsed_time, 'seconds')\n",
        "print(word_wrap(output))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}